[{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/introduction.html","id":"loading-the-package-and-data","dir":"Articles","previous_headings":"","what":"1. Loading the Package and Data","title":"Machine Learning with AddiVortes: A Bayesian Alternative to BART","text":"First, load AddiVortes package. example, use well-known Boston Housing dataset.","code":"# Load the package require(AddiVortes)  # Load the Boston Housing dataset from a URL Boston <- read.csv(paste0(\"https://raw.githubusercontent.com/anonymous2738/\",                           \"AddiVortesAlgorithm/DataSets/BostonHousing_Data.csv\"))   # Separate predictors (X) and the response variable (Y) X_Boston <- as.matrix(Boston[, 2:14]) Y_Boston <- as.numeric(as.matrix(Boston[, 15]))  # Clean up the environment rm(Boston)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/introduction.html","id":"preparing-the-data","dir":"Articles","previous_headings":"","what":"2. Preparing the Data","title":"Machine Learning with AddiVortes: A Bayesian Alternative to BART","text":"evaluate model’s performance, need split data training set testing set. use standard 5/6 split training 1/6 testing.","code":"n <- length(Y_Boston)  # Set a seed for reproducibility set.seed(1025)  # Create a training set containing 5/6 of the data TrainSet <- sort(sample.int(n, 5 * n / 6))  # The remaining data will be our test set TestSet <- setdiff(1:n, TrainSet)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/introduction.html","id":"training-the-model","dir":"Articles","previous_headings":"","what":"3. Training the Model","title":"Machine Learning with AddiVortes: A Bayesian Alternative to BART","text":"Now can run main AddiVortes function training data. specify several parameters algorithm, number iterations trees.","code":"# Run the AddiVortes algorithm on the training data results <- AddiVortes(y = Y_Boston[TrainSet],                        x = X_Boston[TrainSet, ],                       m = 200,                        totalMCMCIter = 2000,                        mcmcBurnIn = 200,                        nu = 6,                        q = 0.85,                        k = 3,                        sd = 0.8,                        omega = 3,                        lambdaRate = 25,                       IntialSigma = \"Linear\",                       showProgress = FALSE)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/introduction.html","id":"making-predictions-and-evaluating-performance","dir":"Articles","previous_headings":"","what":"4. Making Predictions and Evaluating Performance","title":"Machine Learning with AddiVortes: A Bayesian Alternative to BART","text":"trained model object, can now make predictions unseen test data. calculate Root Mean Squared Error (RMSE) see well model performed.","code":"# Generate predictions on the test set preds <- predict(results,                  X_Boston[TestSet, ],                  showProgress = FALSE)  # The RMSE is contained in the results object cat(\"In-Sample RMSE:\", results$inSampleRmse, \"\\n\") #> In-Sample RMSE: 1.185435  # Calculate the Root Mean Squared Error (RMSE) for the test set rmse <- sqrt(mean((Y_Boston[TestSet] - preds)^2)) cat(\"Test Set RMSE:\", rmse, \"\\n\") #> Test Set RMSE: 3.060899"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/introduction.html","id":"visualising-the-results","dir":"Articles","previous_headings":"","what":"5. Visualising the Results","title":"Machine Learning with AddiVortes: A Bayesian Alternative to BART","text":"Finally, good way assess model plot predicted values true values. perfect model, points lie equality line (y = x). also plot prediction intervals visualise model’s uncertainty.","code":"# Plot true values vs. predicted values plot(Y_Boston[TestSet],      preds,      xlab = \"True Values\",      ylab = \"Predicted Values\",      main = \"AddiVortes Predictions vs True Values\",      xlim = range(c(Y_Boston[TestSet], preds)),      ylim = range(c(Y_Boston[TestSet], preds)),      pch = 19, col = \"darkblue\" )  # Add the line of equality (y = x) for reference abline(a = 0, b = 1, col = \"darkred\", lwd = 2)  # Get quantile predictions to create error bars/intervals preds_quantile <- predict(results,                           X_Boston[TestSet, ],                           \"quantile\",                           showProgress = FALSE)  # Add error segments for each prediction for (i in 1:nrow(preds_quantile)) {   segments(Y_Boston[TestSet][i], preds_quantile[i, 1],            Y_Boston[TestSet][i], preds_quantile[i, 2],            col = \"darkblue\", lwd = 1   ) } legend(\"topleft\", legend=c(\"Prediction\", \"y=x Line\", \"95% Interval\"),         col=c(\"darkblue\", \"darkred\", \"darkblue\"),        lty=1, pch=c(19, NA, NA), lwd=2)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/prediction.html","id":"generating-a-synthetic-dataset","dir":"Articles","previous_headings":"","what":"1. Generating a synthetic dataset","title":"Bayesian Regression and Prediction with AddiVortes","text":"start creating 5-dimensional dataset. response variable Y determined simple rule based first two predictors, X[,1] X[,2], plus random noise. allows us know “true” data-generating process, useful evaluating model.","code":"# Load the package library(AddiVortes)  # --- Generate Training Data --- set.seed(42) # for reproducibility  # Create a 5-column matrix of predictors X <- matrix(runif(2500), ncol = 5) X[,1] <- -10 - X[,1] * 10 X[,2] <- X[,2] * 100 X[,3] <- -9 + X[,3] * 10 X[,4] <- 8 + X[,4] X[,5] <- X[,5] * 10  # Create the response 'Y' based on a rule and add noise Y_underlying <- ifelse(-1 * X[,2] > 10 * X[,1] + 100, 10, 0) Y <- Y_underlying + rnorm(length(Y_underlying))  # Visualise the relationships in the data # The colours show the two underlying groups pairs(X,        col = ifelse(Y_underlying == 10, \"red\", \"blue\"),       pch = 19, cex = 0.5,       main = \"Structure of Predictor Variables\")"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/prediction.html","id":"fitting-the-addivortes-model","dir":"Articles","previous_headings":"","what":"2. Fitting the AddiVortes Model","title":"Bayesian Regression and Prediction with AddiVortes","text":"Next, fit AddiVortes model training data. example, ’ll use small number trees (m=50) quick demonstration. -sample RMSE gives us idea well model fits data trained . However, true test model performance new data.","code":"# Fit the model AModel <- AddiVortes(Y, X, m = 50, showProgress = FALSE) # We can check the in-sample Root Mean Squared Error cat(\"In-sample RMSE:\", AModel$inSampleRmse, \"\\n\") #> In-sample RMSE: 1.04841"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/prediction.html","id":"out-of-sample-prediction","dir":"Articles","previous_headings":"","what":"3. Out-of-Sample Prediction","title":"Bayesian Regression and Prediction with AddiVortes","text":"evaluate model’s predictive power, generate new “--sample” test set using process . use predict() method fitted model (AModel) get predictions new data.","code":"# --- Generate Test Data --- set.seed(101) # Use a different seed for the test set testX <- matrix(runif(1000), ncol = 5) testX[,1] <- -10 - testX[,1] * 10 testX[,2] <- testX[,2] * 100 testX[,3] <- -9 + testX[,3] * 10 testX[,4] <- 8 + testX[,4] testX[,5] <- testX[,5] * 10  # Create the true test response values testY_underlying <- ifelse(-1 * testX[,2] > 10 * testX[,1] + 100, 10, 0) testY <- testY_underlying + rnorm(length(testY_underlying))  # --- Make Predictions --- # Predict the mean response preds <- predict(AModel, testX,                  showProgress = FALSE)  # Predict the 90% credible interval (from 0.05 to 0.95 quantiles) # By default, this uses interval = \"confidence\" which only accounts for # uncertainty in the mean function preds_q <- predict(AModel, testX,                    \"quantile\", c(0.05, 0.95), showProgress = FALSE)  # For prediction intervals that also include the model's error variance # (similar to lm's prediction intervals), use interval = \"prediction\" # preds_q_pred <- predict(AModel, testX, #                         \"quantile\", c(0.05, 0.95),  #                         interval = \"prediction\", #                         showProgress = FALSE)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/prediction.html","id":"visualising-prediction-performance","dir":"Articles","previous_headings":"","what":"4. Visualising Prediction Performance","title":"Bayesian Regression and Prediction with AddiVortes","text":"plot effective way visualise model’s performance. plot observed values test set model’s predicted mean values. red lines plot represent true, underlying mean values (0 10) data-generating process. good model produce predictions cluster around lines. blue vertical segments represent 90% credible intervals prediction, giving us sense model’s uncertainty.  plot shows model successfully learned underlying structure data predictions clustering correctly around true mean values 0 10.","code":"# Plot observed vs. predicted values plot(testY,      preds,      xlab = \"Observed Values\",      ylab = \"Predicted Mean Values\",      main = \"Out-of-Sample Prediction Performance\",      xlim = range(c(testY, preds_q)),      ylim = range(c(testY, preds_q)),      pch = 19, col = \"darkblue\" )  # Add error lines for the 90% credible interval for (i in 1:nrow(preds_q)) {   segments(testY[i], preds_q[i, 1],            testY[i], preds_q[i, 2],            col = rgb(0, 0, 0.5, 0.5), lwd = 1.5   ) }  # Add lines showing the true underlying means lines(c(min(testY)-0.2, 3), c(0, 0), col = \"pink\", lwd = 3, lty = 2) lines(c(7, max(testY)+0.2), c(10, 10), col = \"pink\", lwd = 3, lty = 2)  # Add a legend legend(\"bottomright\",        legend = c(\"Predicted Mean & 90% Interval\",                   \"True Underlying Mean\"),        col = c(\"darkblue\", \"pink\"),        pch = c(19, NA),        lty = c(1, 2),        lwd = c(1.5, 3),        bty = \"n\" )"},{"path":"https://johnpaulgosling.github.io/AddiVortes/articles/prediction.html","id":"confidence-intervals-vs--prediction-intervals","dir":"Articles","previous_headings":"","what":"5. Confidence Intervals vs. Prediction Intervals","title":"Bayesian Regression and Prediction with AddiVortes","text":"predict() method supports two types intervals, similar R’s built-lm function: Confidence intervals (interval = \"confidence\", default): reflect uncertainty mean function E[Y|X]. account posterior sampling variability uncertainty tessellation structure, include inherent noise individual observations. Prediction intervals (interval = \"prediction\"): include uncertainty mean function model’s error variance (σ²). makes appropriate predicting individual future observations. Let’s compare two types intervals subset test data:  can see plot, prediction intervals (red) wider confidence intervals (blue). prediction intervals account : Epistemic uncertainty: uncertainty true mean function (captured confidence interval) Aleatoric uncertainty: inherent random noise individual observations (additional width prediction intervals) can quantify difference: use type: Use confidence intervals want understand uncertainty mean response given set predictor values (e.g., “average outcome type case?”) Use prediction intervals want predict individual future observations (e.g., “range outcomes expect specific new case?”) distinction standard regression lm, AddiVortes additionally captures complex non-linear relationships tessellation-based approach.","code":"# Use a subset of test data for clearer visualization subset_indices <- 1:20 testX_subset <- testX[subset_indices, ] testY_subset <- testY[subset_indices]  # Get confidence intervals (uncertainty in mean only) conf_intervals <- predict(AModel, testX_subset,                          type = \"quantile\",                          interval = \"confidence\",                          quantiles = c(0.025, 0.975),                          showProgress = FALSE)  # Get prediction intervals (includes error variance) pred_intervals <- predict(AModel, testX_subset,                          type = \"quantile\",                          interval = \"prediction\",                          quantiles = c(0.025, 0.975),                          showProgress = FALSE)  # Get mean predictions mean_preds <- predict(AModel, testX_subset,                      type = \"response\",                      showProgress = FALSE)  # Create comparison plot plot(1:length(testY_subset), testY_subset,      ylim = range(c(testY_subset, pred_intervals)),      xlab = \"Observation Index\",      ylab = \"Response Value\",      main = \"Confidence Intervals vs. Prediction Intervals\",      pch = 19, col = \"black\", cex = 1.2)  # Add mean predictions points(1:length(testY_subset), mean_preds, pch = 4, col = \"blue\", cex = 1)  # Add confidence intervals (narrower, in blue) for (i in 1:length(testY_subset)) {   segments(i, conf_intervals[i, 1],            i, conf_intervals[i, 2],            col = \"blue\", lwd = 2) }  # Add prediction intervals (wider, in red) for (i in 1:length(testY_subset)) {   segments(i - 0.1, pred_intervals[i, 1],            i - 0.1, pred_intervals[i, 2],            col = \"red\", lwd = 2, lty = 1) }  # Add legend legend(\"topright\",        legend = c(\"Observed Values\", \"Mean Prediction\",                   \"95% Confidence Interval\", \"95% Prediction Interval\"),        col = c(\"black\", \"blue\", \"blue\", \"red\"),        pch = c(19, 4, NA, NA),        lty = c(NA, NA, 1, 1),        lwd = c(NA, NA, 2, 2),        bty = \"n\") # Calculate average interval widths conf_width <- mean(conf_intervals[, 2] - conf_intervals[, 1]) pred_width <- mean(pred_intervals[, 2] - pred_intervals[, 1])  cat(\"Average 95% confidence interval width:\", round(conf_width, 2), \"\\n\") #> Average 95% confidence interval width: 3.05 cat(\"Average 95% prediction interval width:\", round(pred_width, 2), \"\\n\") #> Average 95% prediction interval width: 5.8 cat(\"Ratio (prediction/confidence):\", round(pred_width / conf_width, 2), \"\\n\") #> Ratio (prediction/confidence): 1.9"},{"path":"https://johnpaulgosling.github.io/AddiVortes/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Adam Stone. Author. John Paul Gosling. Author, maintainer.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Stone , Gosling J (2025). AddiVortes: (Bayesian) Additive Voronoi Tessellations. R package version 0.4.2, https://johnpaulgosling.github.io/AddiVortes/.","code":"@Manual{,   title = {AddiVortes: (Bayesian) Additive Voronoi Tessellations},   author = {Adam Stone and John Paul Gosling},   year = {2025},   note = {R package version 0.4.2},   url = {https://johnpaulgosling.github.io/AddiVortes/}, }"},{"path":[]},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"AddiVortes implements Bayesian Additive Voronoi Tessellation model machine learning regression non-parametric statistical modeling. R package provides flexible alternative BART (Bayesian Additive Regression Trees), using Voronoi tessellations instead trees spatial partitioning.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key Features","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"Machine Learning Regression: Advanced Bayesian regression modeling complex datasets Alternative BART: Uses Voronoi tessellations instead trees flexible spatial modeling Spatial Data Analysis: Excellent geographic spatial datasets Non-parametric Modeling: assumptions functional form Bayesian Framework: Full posterior inference uncertainty quantification Complex Function Approximation: Captures non-linear relationships interactions","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"applications","dir":"","previous_headings":"","what":"Applications","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"AddiVortes particularly well-suited : Spatial regression geographic data analysis Machine learning tasks requiring interpretable models Non-parametric regression functional form unknown Bayesian modeling uncertainty quantification Complex surface modeling function approximation Alternative BART researchers seeking different ensemble approaches","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"can install latest version AddiVortes GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"johnpaulgosling/AddiVortes\",                           build_vignettes = TRUE)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick Start","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"","code":"library(AddiVortes)  # Load your data # X <- your_predictors # y <- your_response  # Fit the AddiVortes model # model <- AddiVortesFit(X, y)  # Make predictions # predictions <- predict(model, newdata = X_test)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"Getting Started Guide Prediction Examples Function Reference","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"comparison-with-bart","dir":"","previous_headings":"","what":"Comparison with BART","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"BART (Bayesian Additive Regression Trees) uses tree-based partitioning, AddiVortes uses Voronoi tessellations, can provide: natural spatial partitioning Flexible geometric boundaries Alternative ensemble approach machine learning Enhanced performance spatial data","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"cite-us","dir":"","previous_headings":"","what":"Cite Us","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"use package research, please cite:","code":"citation(\"AddiVortes\")"},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"Stone, . Gosling, J.P. (2025). AddiVortes: (Bayesian) additive Voronoi tessellations. Journal Computational Graphical Statistics.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/index.html","id":"keywords","dir":"","previous_headings":"","what":"Keywords","title":"AddiVortes: Bayesian Machine Learning Alternative to BART","text":"Bayesian machine learning, BART alternative, Voronoi tessellation, spatial regression, non-parametric regression, ensemble methods, statistical modeling, R package","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes-package.html","id":null,"dir":"Reference","previous_headings":"","what":"AddiVortes: Bayesian Additive Voronoi Tessellations for Machine Learning — AddiVortes-package","title":"AddiVortes: Bayesian Additive Voronoi Tessellations for Machine Learning — AddiVortes-package","text":"AddiVortes implements Bayesian Additive Voronoi Tessellation models machine learning regression non-parametric statistical modeling. package provides flexible alternative BART (Bayesian Additive Regression Trees), using Voronoi tessellations instead trees spatial partitioning. method particularly effective spatial data analysis, complex function approximation, Bayesian regression modeling.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"AddiVortes: Bayesian Additive Voronoi Tessellations for Machine Learning — AddiVortes-package","text":"Key features include: Machine learning regression Bayesian inference Alternative BART using Voronoi tessellations Spatial data analysis modeling Non-parametric regression capabilities Complex function approximation Uncertainty quantification posterior inference","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"AddiVortes: Bayesian Additive Voronoi Tessellations for Machine Learning — AddiVortes-package","text":"Stone, . Gosling, J.P. (2025). AddiVortes: (Bayesian) additive Voronoi tessellations. Journal Computational Graphical Statistics.","code":""},{"path":[]},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"AddiVortes: Bayesian Additive Voronoi Tessellations for Machine Learning — AddiVortes-package","text":"Maintainer: John Paul Gosling john-paul.gosling@durham.ac.uk (ORCID) Authors: Adam Stone adam.stone2@durham.ac.uk (ORCID)","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes.html","id":null,"dir":"Reference","previous_headings":"","what":"AddiVortes — AddiVortes","title":"AddiVortes — AddiVortes","text":"AddiVortes function Bayesian nonparametric regression model uses tessellation model relationship covariates output values. model uses backfitting algorithm sample posterior distribution output values tessellation. function returns RMSE value test samples.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AddiVortes — AddiVortes","text":"","code":"AddiVortes(   y,   x,   m = 200,   totalMCMCIter = 1200,   mcmcBurnIn = 200,   nu = 6,   q = 0.85,   k = 3,   sd = 0.8,   omega = 3,   lambdaRate = 25,   IntialSigma = \"Linear\",   thinning = 1,   showProgress = TRUE )"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AddiVortes — AddiVortes","text":"y vector output values. x matrix covariates. m number tessellations. totalMCMCIter number iterations. mcmcBurnIn number burn iterations. nu degrees freedom. q quantile. k number centres. sd standard deviation. omega prior probability adding dimension. lambdaRate rate Poisson distribution number centres. IntialSigma method used calculate initial variance. thinning thinning rate. showProgress Logical; TRUE (default), progress bars messages shown fitting.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AddiVortes — AddiVortes","text":"AddiVortesFit object containing posterior samples tessellations, dimensions predictions.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/AddiVortes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AddiVortes — AddiVortes","text":"","code":"# \\donttest{ # Simple example with simulated data set.seed(123) x <- matrix(rnorm(50), 10, 5) y <- rnorm(10) # Fit model with reduced iterations for quick example fit <- AddiVortes(y, x, m = 5, totalMCMCIter = 50, mcmcBurnIn = 10) #> Fitting AddiVortes model to input data... #> Input dimensions: 10 observations, 5 covariates #> Model configuration: 5 tessellations, 50 total iterations (10 burn-in) #>  #> Phase 1: Burn-in sampling (10 iterations) #>    |                                                           |                                                  |   0%   |                                                           |=====                                             |  10%   |                                                           |==========                                        |  20%   |                                                           |===============                                   |  30%   |                                                           |====================                              |  40%   |                                                           |=========================                         |  50%   |                                                           |==============================                    |  60%   |                                                           |===================================               |  70%   |                                                           |========================================          |  80%   |                                                           |=============================================     |  90%   |                                                           |==================================================| 100% #>  #> Phase 2: Posterior sampling (40 iterations) #>    |                                                           |                                                  |   0%   |                                                           |==                                                |   5%   |                                                           |====                                              |   8%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  12%   |                                                           |========                                          |  15%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  25%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  32%   |                                                           |==================                                |  35%   |                                                           |===================                               |  38%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  45%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  50%   |                                                           |==========================                        |  52%   |                                                           |============================                      |  55%   |                                                           |=============================                     |  58%   |                                                           |==============================                    |  60%   |                                                           |===============================                   |  62%   |                                                           |================================                  |  65%   |                                                           |==================================                |  68%   |                                                           |===================================               |  70%   |                                                           |====================================              |  72%   |                                                           |======================================            |  75%   |                                                           |=======================================           |  78%   |                                                           |========================================          |  80%   |                                                           |=========================================         |  82%   |                                                           |==========================================        |  85%   |                                                           |============================================      |  88%   |                                                           |=============================================     |  90%   |                                                           |==============================================    |  92%   |                                                           |================================================  |  95%   |                                                           |================================================= |  98%   |                                                           |==================================================| 100% #>  #> MCMC sampling completed. #>  # }"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/new_AddiVortesFit.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an AddiVortesFit Object — new_AddiVortesFit","title":"Create an AddiVortesFit Object — new_AddiVortesFit","text":"constructor AddiVortesFit class.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/new_AddiVortesFit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an AddiVortesFit Object — new_AddiVortesFit","text":"","code":"new_AddiVortesFit(   posteriorTess,   posteriorDim,   posteriorSigma,   posteriorPred,   xCentres,   xRanges,   yCentre,   yRange,   inSampleRmse )"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/new_AddiVortesFit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an AddiVortesFit Object — new_AddiVortesFit","text":"posteriorTess list posterior samples tessellations. posteriorDim list posterior samples dimensions. posteriorSigma list posterior samples error variance. posteriorPred list posterior samples predictions. xCentres centres covariates. xRanges ranges covariates. yCentre centre output values. yRange range output values. inSampleRmse -sample RMSE.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/new_AddiVortesFit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an AddiVortesFit Object — new_AddiVortesFit","text":"object class AddiVortesFit.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/plot.AddiVortesFit.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for AddiVortesFit — plot.AddiVortesFit","title":"Plot Method for AddiVortesFit — plot.AddiVortesFit","text":"Generates comprehensive diagnostic plots fitted AddiVortesFit object. function creates multiple diagnostic plots including residuals, MCMC traces sigma, tessellation complexity iterations.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/plot.AddiVortesFit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for AddiVortesFit — plot.AddiVortesFit","text":"","code":"# S3 method for class 'AddiVortesFit' plot(   x,   x_train,   y_train,   sigma_trace = NULL,   which = c(1, 2, 3),   ask = FALSE,   ... )"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/plot.AddiVortesFit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for AddiVortesFit — plot.AddiVortesFit","text":"x object class AddiVortesFit, typically result call AddiVortes(). x_train matrix original training covariates. y_train numeric vector original training true outcomes. sigma_trace optional numeric vector sigma values MCMC samples. provided, method attempt extract model object. numeric vector specifying plots generate: 1 = Residuals plot, 2 = Sigma trace, 3 = Tessellation complexity trace, 4 = Predicted vs Observed. Default c(1, 2, 3). ask Logical; TRUE, user asked press Enter plot. ... Additional arguments passed plotting functions.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/plot.AddiVortesFit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Method for AddiVortesFit — plot.AddiVortesFit","text":"function called side effect creating plots returns NULL invisibly.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/plot.AddiVortesFit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Method for AddiVortesFit — plot.AddiVortesFit","text":"function generates four diagnostic plots: Residuals Plot: Residuals vs fitted values smoothed trend line Sigma Trace: MCMC trace plot error variance parameter Tessellation Complexity: Trace average tessellation size iterations Predicted vs Observed: Scatter plot confidence intervals","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/plot.AddiVortesFit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for AddiVortesFit — plot.AddiVortesFit","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'fit' is a trained AddiVortesFit object plot(fit, x_train = x_train_data, y_train = y_train_data)  # Show only specific plots plot(fit, x_train = x_train_data, y_train = y_train_data, which = c(1, 3))  # With custom sigma trace plot(fit, x_train = x_train_data, y_train = y_train_data,       sigma_trace = my_sigma_samples) } # }"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/predict.AddiVortesFit.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for AddiVortesFit — predict.AddiVortesFit","title":"Predict Method for AddiVortesFit — predict.AddiVortesFit","text":"Predicts outcomes new data using fitted AddiVortesFit model object. can return mean predictions, quantiles optionally calculate Root Mean Squared Error (RMSE) true outcomes provided.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/predict.AddiVortesFit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for AddiVortesFit — predict.AddiVortesFit","text":"","code":"# S3 method for class 'AddiVortesFit' predict(   object,   newdata,   type = c(\"response\", \"quantile\"),   quantiles = c(0.025, 0.975),   interval = c(\"confidence\", \"prediction\"),   showProgress = TRUE,   parallel = TRUE,   cores = NULL,   ... )"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/predict.AddiVortesFit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for AddiVortesFit — predict.AddiVortesFit","text":"object object class AddiVortesFit, typically result call AddiVortes(). newdata matrix covariates new test set. number columns must match original training data. type type prediction required. default \"response\" gives mean prediction. alternative \"quantile\" returns quantiles specified quantiles argument. quantiles numeric vector probabilities values 0, 1 compute predictions type = \"quantile\". interval type interval calculation. default \"confidence\" accounts uncertainty mean (similar lm's confidence interval). alternative \"prediction\" also includes model's error variance, producing wider intervals (similar lm's prediction interval). showProgress Logical; TRUE (default), progress bar shown prediction. parallel Logical; TRUE (default), predictions computed parallel. cores number CPU cores use parallel processing. NULL (default), defaults one less total number available cores. ... arguments passed methods (currently unused).","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/predict.AddiVortesFit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for AddiVortesFit — predict.AddiVortesFit","text":"type = \"response\", numeric vector mean predictions. type = \"quantile\", matrix row corresponds observation newdata column quantile.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/predict.AddiVortesFit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for AddiVortesFit — predict.AddiVortesFit","text":"function relies internal helper function applyScaling_internal available environment, used main AddiVortes function. interval = \"prediction\" type = \"quantile\", function samples additional Gaussian noise variance equal sampled sigma squared posterior. accounts inherent variability individual predictions, just uncertainty mean function. noise added scaled space unscaling predictions.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/predict.AddiVortesFit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for AddiVortesFit — predict.AddiVortesFit","text":"","code":"# \\donttest{ # Fit a model set.seed(123) X <- matrix(rnorm(100), 20, 5) Y <- rnorm(20) fit <- AddiVortes(Y, X, m = 5, totalMCMCIter = 50, mcmcBurnIn = 10) #> Fitting AddiVortes model to input data... #> Input dimensions: 20 observations, 5 covariates #> Model configuration: 5 tessellations, 50 total iterations (10 burn-in) #>  #> Phase 1: Burn-in sampling (10 iterations) #>    |                                                           |                                                  |   0%   |                                                           |=====                                             |  10%   |                                                           |==========                                        |  20%   |                                                           |===============                                   |  30%   |                                                           |====================                              |  40%   |                                                           |=========================                         |  50%   |                                                           |==============================                    |  60%   |                                                           |===================================               |  70%   |                                                           |========================================          |  80%   |                                                           |=============================================     |  90%   |                                                           |==================================================| 100% #>  #> Phase 2: Posterior sampling (40 iterations) #>    |                                                           |                                                  |   0%   |                                                           |==                                                |   5%   |                                                           |====                                              |   8%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  12%   |                                                           |========                                          |  15%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  25%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  32%   |                                                           |==================                                |  35%   |                                                           |===================                               |  38%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  45%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  50%   |                                                           |==========================                        |  52%   |                                                           |============================                      |  55%   |                                                           |=============================                     |  58%   |                                                           |==============================                    |  60%   |                                                           |===============================                   |  62%   |                                                           |================================                  |  65%   |                                                           |==================================                |  68%   |                                                           |===================================               |  70%   |                                                           |====================================              |  72%   |                                                           |======================================            |  75%   |                                                           |=======================================           |  78%   |                                                           |========================================          |  80%   |                                                           |=========================================         |  82%   |                                                           |==========================================        |  85%   |                                                           |============================================      |  88%   |                                                           |=============================================     |  90%   |                                                           |==============================================    |  92%   |                                                           |================================================  |  95%   |                                                           |================================================= |  98%   |                                                           |==================================================| 100% #>  #> MCMC sampling completed. #>   # New data for prediction X_new <- matrix(rnorm(25), 5, 5)  # Mean predictions pred_mean <- predict(fit, X_new, type = \"response\") #> Generating predictions for 5 observations using 40 posterior samples... #>  #> Prediction generation completed. #>   # Confidence intervals (uncertainty in mean only) pred_conf <- predict(fit, X_new, type = \"quantile\",                      interval = \"confidence\",                     quantiles = c(0.025, 0.975)) #> Generating predictions for 5 observations using 40 posterior samples... #>  #> Prediction generation completed. #>   # Prediction intervals (includes error variance) pred_pred <- predict(fit, X_new, type = \"quantile\",                     interval = \"prediction\",                     quantiles = c(0.025, 0.975)) #> Generating predictions for 5 observations using 40 posterior samples... #>  #> Prediction generation completed. #>   # Prediction intervals are wider than confidence intervals mean(pred_pred[, 2] - pred_pred[, 1]) > mean(pred_conf[, 2] - pred_conf[, 1]) #> [1] TRUE # }"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/print.AddiVortesFit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for AddiVortesFit — print.AddiVortesFit","title":"Print Method for AddiVortesFit — print.AddiVortesFit","text":"Prints summary fitted AddiVortesFit object, providing information model structure, dimensions, fit quality similar output linear model summary.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/print.AddiVortesFit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for AddiVortesFit — print.AddiVortesFit","text":"","code":"# S3 method for class 'AddiVortesFit' print(x, ...)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/print.AddiVortesFit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for AddiVortesFit — print.AddiVortesFit","text":"x object class AddiVortesFit, typically result call AddiVortes(). ... arguments passed methods (currently unused).","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/print.AddiVortesFit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Method for AddiVortesFit — print.AddiVortesFit","text":"function called side effect printing model information returns input object x invisibly.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/print.AddiVortesFit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Print Method for AddiVortesFit — print.AddiVortesFit","text":"print method displays: model formula representation Number covariates posterior samples Number tessellations used -sample RMSE Covariate scaling information","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/summary.AddiVortesFit.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Method for AddiVortesFit — summary.AddiVortesFit","title":"Summary Method for AddiVortesFit — summary.AddiVortesFit","text":"Provides detailed summary fitted AddiVortesFit object, including comprehensive information print method.","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/summary.AddiVortesFit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Method for AddiVortesFit — summary.AddiVortesFit","text":"","code":"# S3 method for class 'AddiVortesFit' summary(object, ...)"},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/summary.AddiVortesFit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Method for AddiVortesFit — summary.AddiVortesFit","text":"object object class AddiVortesFit, typically result call AddiVortes(). ... arguments passed methods (currently unused).","code":""},{"path":"https://johnpaulgosling.github.io/AddiVortes/reference/summary.AddiVortesFit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Method for AddiVortesFit — summary.AddiVortesFit","text":"function called side effect printing detailed model information returns input object object invisibly.","code":""}]
